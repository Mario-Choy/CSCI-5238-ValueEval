{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a708042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531f8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.3.0\n",
      "accelerate==0.15.0\n",
      "aiohttp==3.8.3\n",
      "aiosignal==1.3.1\n",
      "astunparse==1.6.3\n",
      "async-timeout==4.0.2\n",
      "asynctest==0.13.0\n",
      "attrs==22.1.0\n",
      "backcall==0.2.0\n",
      "cachetools==5.2.0\n",
      "certifi==2022.9.24\n",
      "charset-normalizer==2.1.1\n",
      "colorama==0.4.6\n",
      "cycler==0.11.0\n",
      "datasets==2.7.1\n",
      "debugpy==1.6.3\n",
      "decorator==5.1.1\n",
      "dill==0.3.6\n",
      "entrypoints==0.4\n",
      "evaluate==0.3.0\n",
      "filelock==3.8.0\n",
      "flatbuffers==22.11.23\n",
      "fonttools==4.38.0\n",
      "frozenlist==1.3.3\n",
      "fsspec==2022.11.0\n",
      "gast==0.4.0\n",
      "google-auth==2.15.0\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.51.1\n",
      "h5py==3.7.0\n",
      "htmlmin==0.1.12\n",
      "huggingface-hub==0.11.1\n",
      "idna==3.4\n",
      "ImageHash==4.3.1\n",
      "importlib-metadata==5.1.0\n",
      "ipykernel==6.16.2\n",
      "ipython==7.34.0\n",
      "ipywidgets==8.0.2\n",
      "jedi==0.18.2\n",
      "Jinja2==3.1.2\n",
      "joblib==1.2.0\n",
      "jupyter_client==7.4.7\n",
      "jupyter_core==4.11.2\n",
      "jupyterlab-widgets==3.0.3\n",
      "keras==2.10.0\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.4.4\n",
      "libclang==14.0.6\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.1\n",
      "matplotlib==3.5.3\n",
      "matplotlib-inline==0.1.6\n",
      "multidict==6.0.3\n",
      "multimethod==1.9\n",
      "multiprocess==0.70.14\n",
      "nest-asyncio==1.5.6\n",
      "networkx==2.6.3\n",
      "numpy==1.21.6\n",
      "nvidia-ml-py3==7.352.0\n",
      "oauthlib==3.2.2\n",
      "opt-einsum==3.3.0\n",
      "packaging==21.3\n",
      "pandas==1.3.5\n",
      "pandas-profiling==3.5.0\n",
      "parso==0.8.3\n",
      "patsy==0.5.3\n",
      "phik==0.12.2\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.3.0\n",
      "pip-chill==1.0.1\n",
      "prompt-toolkit==3.0.33\n",
      "protobuf==3.19.6\n",
      "psutil==5.9.4\n",
      "pyarrow==10.0.1\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pydantic==1.10.2\n",
      "Pygments==2.13.0\n",
      "pyparsing==3.0.9\n",
      "python-dateutil==2.8.2\n",
      "pytz==2022.6\n",
      "PyWavelets==1.3.0\n",
      "pywin32==305\n",
      "PyYAML==6.0\n",
      "pyzmq==24.0.1\n",
      "regex==2022.10.31\n",
      "requests==2.28.1\n",
      "requests-oauthlib==1.3.1\n",
      "responses==0.18.0\n",
      "rsa==4.9\n",
      "scikit-learn==1.0.2\n",
      "scipy==1.7.3\n",
      "seaborn==0.12.1\n",
      "six==1.16.0\n",
      "statsmodels==0.13.5\n",
      "tangled-up-in-unicode==0.2.0\n",
      "tensorboard==2.10.1\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow-estimator==2.10.0\n",
      "tensorflow-io-gcs-filesystem==0.28.0\n",
      "termcolor==2.1.1\n",
      "threadpoolctl==3.1.0\n",
      "tokenizers==0.13.2\n",
      "torch==1.13.0+cu116\n",
      "torchaudio==0.13.0+cu116\n",
      "torchvision==0.14.0+cu116\n",
      "tornado==6.2\n",
      "tqdm==4.64.1\n",
      "traitlets==5.5.0\n",
      "transformers==4.25.1\n",
      "typeguard==2.13.3\n",
      "typing_extensions==4.4.0\n",
      "urllib3==1.26.13\n",
      "visions==0.7.5\n",
      "wcwidth==0.2.5\n",
      "Werkzeug==2.2.2\n",
      "widgetsnbextension==4.0.3\n",
      "wrapt==1.14.1\n",
      "xxhash==3.1.0\n",
      "yarl==1.8.2\n",
      "zipp==3.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\aorlowski\\documents\\code\\cu\\csci-5832-valueeval\\venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54246a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Argument ID                      Conclusion       Stance  \\\n",
      "0         A01001  Entrapment should be legalized  in favor of   \n",
      "1         A01002     We should ban human cloning  in favor of   \n",
      "2         A01003      We should abandon marriage      against   \n",
      "3         A01004       We should ban naturopathy      against   \n",
      "4         A01005         We should ban fast food  in favor of   \n",
      "...          ...                             ...          ...   \n",
      "5215      D27096    Nepotism exists in Bollywood      against   \n",
      "5216      D27097    Nepotism exists in Bollywood  in favor of   \n",
      "5217      D27098         India is safe for women  in favor of   \n",
      "5218      D27099         India is safe for women  in favor of   \n",
      "5219      D27100         India is safe for women      against   \n",
      "\n",
      "                                                Premise  \n",
      "0     if entrapment can serve to more easily capture...  \n",
      "1     we should ban human cloning as it will only ca...  \n",
      "2     marriage is the ultimate commitment to someone...  \n",
      "3           it provides a useful income for some people  \n",
      "4     fast food should be banned because it is reall...  \n",
      "...                                                 ...  \n",
      "5215  Star kids also have an upbringing which is sur...  \n",
      "5216  Movie stars of Bollywood often launch their ch...  \n",
      "5217  Evil historic practices on women in the pre an...  \n",
      "5218  Women of our country have been and are achievi...  \n",
      "5219  The National Crime Records Bureau states that ...  \n",
      "\n",
      "[5220 rows x 4 columns]\n",
      "     Argument ID  Self-direction: thought  Self-direction: action  \\\n",
      "0         A01001                        0                       0   \n",
      "1         A01002                        0                       0   \n",
      "2         A01003                        0                       1   \n",
      "3         A01004                        0                       0   \n",
      "4         A01005                        0                       0   \n",
      "...          ...                      ...                     ...   \n",
      "5215      D27096                        1                       0   \n",
      "5216      D27097                        0                       0   \n",
      "5217      D27098                        0                       0   \n",
      "5218      D27099                        0                       0   \n",
      "5219      D27100                        0                       0   \n",
      "\n",
      "      Stimulation  Hedonism  Achievement  Power: dominance  Power: resources  \\\n",
      "0               0         0            0                 0                 0   \n",
      "1               0         0            0                 0                 0   \n",
      "2               0         0            0                 0                 0   \n",
      "3               0         0            0                 0                 0   \n",
      "4               0         0            0                 0                 0   \n",
      "...           ...       ...          ...               ...               ...   \n",
      "5215            0         0            1                 0                 0   \n",
      "5216            0         0            0                 0                 0   \n",
      "5217            0         0            0                 0                 0   \n",
      "5218            0         0            1                 0                 0   \n",
      "5219            0         0            0                 0                 0   \n",
      "\n",
      "      Face  Security: personal  ...  Tradition  Conformity: rules  \\\n",
      "0        0                   0  ...          0                  0   \n",
      "1        0                   0  ...          0                  0   \n",
      "2        0                   0  ...          0                  0   \n",
      "3        0                   1  ...          0                  0   \n",
      "4        0                   1  ...          0                  0   \n",
      "...    ...                 ...  ...        ...                ...   \n",
      "5215     1                   0  ...          0                  0   \n",
      "5216     0                   0  ...          0                  0   \n",
      "5217     0                   0  ...          0                  0   \n",
      "5218     0                   0  ...          0                  0   \n",
      "5219     0                   0  ...          0                  1   \n",
      "\n",
      "      Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                             0         0                    0   \n",
      "1                             0         0                    0   \n",
      "2                             0         0                    0   \n",
      "3                             0         0                    0   \n",
      "4                             0         0                    0   \n",
      "...                         ...       ...                  ...   \n",
      "5215                          0         0                    0   \n",
      "5216                          0         0                    1   \n",
      "5217                          0         0                    0   \n",
      "5218                          0         0                    0   \n",
      "5219                          0         0                    0   \n",
      "\n",
      "      Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                              0                      0                     0   \n",
      "1                              0                      0                     0   \n",
      "2                              0                      0                     0   \n",
      "3                              0                      0                     0   \n",
      "4                              0                      0                     0   \n",
      "...                          ...                    ...                   ...   \n",
      "5215                           0                      0                     0   \n",
      "5216                           0                      0                     0   \n",
      "5217                           0                      0                     0   \n",
      "5218                           0                      0                     0   \n",
      "5219                           0                      1                     0   \n",
      "\n",
      "      Universalism: tolerance  Universalism: objectivity  \n",
      "0                           0                          0  \n",
      "1                           0                          0  \n",
      "2                           0                          0  \n",
      "3                           0                          0  \n",
      "4                           0                          0  \n",
      "...                       ...                        ...  \n",
      "5215                        0                          0  \n",
      "5216                        0                          0  \n",
      "5217                        0                          1  \n",
      "5218                        0                          1  \n",
      "5219                        0                          1  \n",
      "\n",
      "[5220 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load Arguments Dataset\n",
    "data_folder = './data/'\n",
    "arguments_file = 'arguments-training.tsv'\n",
    "labels_file = 'labels-training.tsv'\n",
    "arguments_train_df = pd.read_csv(os.path.join(data_folder, arguments_file), encoding='utf-8', sep='\\t', header=0)\n",
    "labels_train_df = pd.read_csv(os.path.join(data_folder, labels_file), encoding='utf-8', sep='\\t', header=0)\n",
    "\n",
    "print(arguments_train_df)\n",
    "print(labels_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba0b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 4176\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__'],\n",
      "        num_rows: 1044\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine the columsn in arguments to be a single field to give to bert\n",
    "\n",
    "# Inputs: \n",
    "# an argument df from the source data (ArgumentId, Conclusion, Stance, Premise). \n",
    "# Labels df from file. \n",
    "# Name of label that will be trained on.\n",
    "\n",
    "# Returns: df with a single column of arguments that is Conclusion: Conclusion, Stance: stance, Premise: Premise \n",
    "# along with the labels\n",
    "def setup_train_df(arguments_df, labels_df, target_label):\n",
    "    arguments_df['text'] = 'Conclusion: ' + arguments_df['Conclusion'] + ', Stance: ' + arguments_df['Stance'] + ', Premise: ' + arguments_df['Premise']\n",
    "    resp = arguments_df.filter(['text'], axis=1)\n",
    "    resp['label'] = labels_df[target_label]\n",
    "    return resp\n",
    "\n",
    "# This is where the specific value label is selected.\n",
    "target_label = 'Achievement'\n",
    "train = setup_train_df(arguments_train_df, labels_train_df, target_label)\n",
    "train, test = train_test_split(train, test_size=0.2)\n",
    "dataset = datasets.DatasetDict({\"train\":Dataset.from_pandas(train),\"test\":Dataset.from_pandas(test)})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24130771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c98f77f9f684e9da547025fe7b1c2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7620419eeebc4f369f5eb6527c0b1142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4176\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 1044\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Do some huggingface/transformers setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_arguments = dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_arguments)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129d336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "# Running this model on GPU https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "# Takes some magic.\n",
    "# Windows instructions:\n",
    "# nvidia-smi should work from cmd\n",
    "# Ended up doing this https://github.com/wookayin/gpustat/issues/90#issuecomment-753591406\n",
    "# The dll name is nvml.dll\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb43d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1557 MB.\n"
     ]
    }
   ],
   "source": [
    "# Do training here\n",
    "# This is based on this guide: https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "# TODO: Split train/test data. Optimizer. Fine tuning. Get the training to run. Evaluation.\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# For to(\"cuda\") to work here, the GPU/CUDA version of torch needs to be installed https://pytorch.org/get-started/locally/\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(\"cuda\")\n",
    "print_gpu_utilization()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_arguments[\"train\"],\n",
    "    eval_dataset=tokenized_arguments[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d932b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\aorlowski\\Documents\\code\\cu\\CSCI-5832-ValueEval\\venv\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1305\n",
      "  Number of trainable parameters = 66955010\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1305 00:00 < 04:02, 5.36 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valueeval",
   "language": "python",
   "name": "valueeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
